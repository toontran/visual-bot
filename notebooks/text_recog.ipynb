{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import argparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "sys.path.insert(0, '../../deep-text-recognition-benchmark/')\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from utils import CTCLabelConverter, AttnLabelConverter\n",
    "from dataset import RawDataset, AlignCollate\n",
    "from model import Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tung/pj/sel/CRAFT-pytorch/notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import craft_utils\n",
    "import test\n",
    "import imgproc\n",
    "import file_utils\n",
    "import json\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "from craft import CRAFT\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    trained_model='../weights/craft_mlt_25k.pth',\n",
    "    text_threshold=0.3,\n",
    "    low_text=0.3,\n",
    "    link_threshold=0.3,\n",
    "    cuda=True,\n",
    "    canvas_size=1280,\n",
    "    mag_ratio=1.5,\n",
    "    poly=False,\n",
    "    show_time=False,\n",
    "    test_folder='../Results/screenshots',\n",
    "    refine=False,\n",
    "    refiner_model='../weights/craft_refiner_CTW1500.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../Results/screenshots/screenshot_1.png', '../Results/screenshots/screenshot_3.png', '../Results/screenshots/screenshot_4.png', '../Results/screenshots/screenshot_0.png', '../Results/screenshots/screenshot_2.png', '../Results/screenshots/.ipynb_checkpoints/screenshot_4-checkpoint.png', '../Results/screenshots/.ipynb_checkpoints/screenshot_0-checkpoint.png', '../Results/screenshots/.ipynb_checkpoints/screenshot_2-checkpoint.png', '../Results/screenshots/.ipynb_checkpoints/screenshot_3-checkpoint.png', '../Results/screenshots/.ipynb_checkpoints/screenshot_1-checkpoint.png']\n",
      "Loading weights from checkpoint (../weights/craft_mlt_25k.pth)\n",
      "Test image 10/10: ../Results/screenshots/.ipynb_checkpoints/screenshot_1-checkpoint.png\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>word_bboxes</th>\n",
       "      <th>pred_words</th>\n",
       "      <th>align_text</th>\n",
       "      <th>image_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>screenshot_1.png</td>\n",
       "      <td>{'0.89525574': [[791.9999, 14.999997], [1112.9...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[[1, 1, 1], [19, 19, 19], [1, 1, 1], [19, 19,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>screenshot_3.png</td>\n",
       "      <td>{'0.6253818': [[1641.0, 15.0], [1683.0, 15.0],...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[[1, 1, 1], [19, 19, 19], [1, 1, 1], [18, 18,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>screenshot_4.png</td>\n",
       "      <td>{'0.6253818': [[1641.0, 15.0], [1683.0, 15.0],...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[[1, 1, 1], [19, 19, 19], [1, 1, 1], [18, 18,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>screenshot_0.png</td>\n",
       "      <td>{'0.8324406': [[1272.0, 12.0], [1389.0, 12.0],...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[[23, 23, 23], [27, 27, 27], [27, 27, 27], [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>screenshot_2.png</td>\n",
       "      <td>{'0.6253818': [[1641.0, 15.0], [1683.0, 15.0],...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[[1, 1, 1], [19, 19, 19], [1, 1, 1], [18, 18,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.ipynb_checkpoints/screenshot_4-checkpoint.png</td>\n",
       "      <td>{'0.73630595': [[219.0, 15.0], [264.0, 15.0], ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[[23, 23, 23], [27, 27, 27], [27, 27, 27], [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.ipynb_checkpoints/screenshot_0-checkpoint.png</td>\n",
       "      <td>{'0.8324406': [[1272.0, 12.0], [1389.0, 12.0],...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[[23, 23, 23], [27, 27, 27], [27, 27, 27], [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.ipynb_checkpoints/screenshot_2-checkpoint.png</td>\n",
       "      <td>{'0.73630595': [[219.0, 15.0], [264.0, 15.0], ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[[23, 23, 23], [27, 27, 27], [27, 27, 27], [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.ipynb_checkpoints/screenshot_3-checkpoint.png</td>\n",
       "      <td>{'0.6253818': [[1641.0, 15.0], [1683.0, 15.0],...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[[1, 1, 1], [19, 19, 19], [1, 1, 1], [18, 18,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.ipynb_checkpoints/screenshot_1-checkpoint.png</td>\n",
       "      <td>{'0.73630595': [[219.0, 15.0], [264.0, 15.0], ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[[23, 23, 23], [27, 27, 27], [27, 27, 27], [2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       image_name  \\\n",
       "0                                screenshot_1.png   \n",
       "1                                screenshot_3.png   \n",
       "2                                screenshot_4.png   \n",
       "3                                screenshot_0.png   \n",
       "4                                screenshot_2.png   \n",
       "5  .ipynb_checkpoints/screenshot_4-checkpoint.png   \n",
       "6  .ipynb_checkpoints/screenshot_0-checkpoint.png   \n",
       "7  .ipynb_checkpoints/screenshot_2-checkpoint.png   \n",
       "8  .ipynb_checkpoints/screenshot_3-checkpoint.png   \n",
       "9  .ipynb_checkpoints/screenshot_1-checkpoint.png   \n",
       "\n",
       "                                         word_bboxes pred_words align_text  \\\n",
       "0  {'0.89525574': [[791.9999, 14.999997], [1112.9...        NaN        NaN   \n",
       "1  {'0.6253818': [[1641.0, 15.0], [1683.0, 15.0],...        NaN        NaN   \n",
       "2  {'0.6253818': [[1641.0, 15.0], [1683.0, 15.0],...        NaN        NaN   \n",
       "3  {'0.8324406': [[1272.0, 12.0], [1389.0, 12.0],...        NaN        NaN   \n",
       "4  {'0.6253818': [[1641.0, 15.0], [1683.0, 15.0],...        NaN        NaN   \n",
       "5  {'0.73630595': [[219.0, 15.0], [264.0, 15.0], ...        NaN        NaN   \n",
       "6  {'0.8324406': [[1272.0, 12.0], [1389.0, 12.0],...        NaN        NaN   \n",
       "7  {'0.73630595': [[219.0, 15.0], [264.0, 15.0], ...        NaN        NaN   \n",
       "8  {'0.6253818': [[1641.0, 15.0], [1683.0, 15.0],...        NaN        NaN   \n",
       "9  {'0.73630595': [[219.0, 15.0], [264.0, 15.0], ...        NaN        NaN   \n",
       "\n",
       "                                          image_data  \n",
       "0  [[[1, 1, 1], [19, 19, 19], [1, 1, 1], [19, 19,...  \n",
       "1  [[[1, 1, 1], [19, 19, 19], [1, 1, 1], [18, 18,...  \n",
       "2  [[[1, 1, 1], [19, 19, 19], [1, 1, 1], [18, 18,...  \n",
       "3  [[[23, 23, 23], [27, 27, 27], [27, 27, 27], [2...  \n",
       "4  [[[1, 1, 1], [19, 19, 19], [1, 1, 1], [18, 18,...  \n",
       "5  [[[23, 23, 23], [27, 27, 27], [27, 27, 27], [2...  \n",
       "6  [[[23, 23, 23], [27, 27, 27], [27, 27, 27], [2...  \n",
       "7  [[[23, 23, 23], [27, 27, 27], [27, 27, 27], [2...  \n",
       "8  [[[1, 1, 1], [19, 19, 19], [1, 1, 1], [18, 18,...  \n",
       "9  [[[23, 23, 23], [27, 27, 27], [27, 27, 27], [2...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list, _, _ = file_utils.get_files(args.test_folder)\n",
    "\n",
    "print(image_list)\n",
    "\n",
    "image_names = []\n",
    "image_paths = []\n",
    "\n",
    "#CUSTOMISE START\n",
    "start = args.test_folder\n",
    "\n",
    "for num in range(len(image_list)):\n",
    "    image_names.append(os.path.relpath(image_list[num], start))\n",
    "\n",
    "\n",
    "result_folder = '../Results'\n",
    "if not os.path.isdir(result_folder):\n",
    "    os.mkdir(result_folder)\n",
    "\n",
    "data=pd.DataFrame(columns=['image_name', 'word_bboxes', 'pred_words', 'align_text'])\n",
    "data['image_name'] = image_names\n",
    "data[\"image_data\"] = [None for x in image_names]\n",
    "\n",
    "# load net\n",
    "net = CRAFT()     # initialize\n",
    "\n",
    "print('Loading weights from checkpoint (' + args.trained_model + ')')\n",
    "if args.cuda:\n",
    "    net.load_state_dict(test.copyStateDict(torch.load(args.trained_model)))\n",
    "else:\n",
    "    net.load_state_dict(test.copyStateDict(torch.load(args.trained_model, map_location='cpu')))\n",
    "\n",
    "if args.cuda:\n",
    "    net = net.cuda()\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = False\n",
    "\n",
    "net.eval()\n",
    "\n",
    "# LinkRefiner\n",
    "refine_net = None\n",
    "if args.refine:\n",
    "    from refinenet import RefineNet\n",
    "    refine_net = RefineNet()\n",
    "    print('Loading weights of refiner from checkpoint (' + args.refiner_model + ')')\n",
    "    if args.cuda:\n",
    "        refine_net.load_state_dict(torch.load(args.refiner_model))\n",
    "        refine_net = refine_net.cuda()\n",
    "        refine_net = torch.nn.DataParallel(refine_net)\n",
    "    else:\n",
    "        refine_net.load_state_dict(copyStateDict(torch.load(args.refiner_model, map_location='cpu')))\n",
    "\n",
    "    refine_net.eval()\n",
    "    args.poly = True\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "# load data\n",
    "for k, image_path in enumerate(image_list):\n",
    "    print(\"Test image {:d}/{:d}: {:s}\".format(k+1, len(image_list), image_path), end='\\r')\n",
    "    image = imgproc.loadImage(image_path)\n",
    "    data[\"image_data\"][k] = image\n",
    "#     image = image[:54,:]\n",
    "\n",
    "    bboxes, polys, score_text, det_scores = test.test_net(net, image, args.text_threshold, args.link_threshold, args.low_text, args.cuda, args.poly, args, refine_net)\n",
    "\n",
    "    bbox_score={}\n",
    "\n",
    "    for box_num in range(len(bboxes)):\n",
    "        key = str(det_scores[box_num])\n",
    "        item = bboxes[box_num]\n",
    "        bbox_score[key]=item\n",
    "\n",
    "    data['word_bboxes'][k]=bbox_score\n",
    "    # save score text\n",
    "    filename, file_ext = os.path.splitext(os.path.basename(image_path))\n",
    "    mask_file = result_folder + \"/res_\" + filename + '_mask.jpg'\n",
    "    cv2.imwrite(mask_file, score_text)\n",
    "\n",
    "    file_utils.saveResult(image_path, image[:,:,::-1], polys, dirname=result_folder)\n",
    "\n",
    "# data.to_csv(result_folder + 'data.csv', sep = ',', na_rep='Unknown')\n",
    "# print(\"elapsed time : {}s\".format(time.time() - t))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = argparse.Namespace(\n",
    "    image_folder=\"/home/tung/pj/sel/CRAFT-pytorch/Results/cropped\",\n",
    "    workers=4,\n",
    "    batch_size=192,\n",
    "    saved_model=\"/home/tung/pj/sel/deep-text-recognition-benchmark/weights/TPS-ResNet-BiLSTM-CTC.pth\",\n",
    "    batch_max_length=25,\n",
    "    imgH=32,\n",
    "    imgW=100,\n",
    "    rgb=False,\n",
    "    character='0123456789abcdefghijklmnopqrstuvwxyz',\n",
    "    sensitive=False,\n",
    "    PAD=True,\n",
    "    Transformation=\"TPS\",\n",
    "    FeatureExtraction=\"ResNet\",\n",
    "    SequenceModeling=\"BiLSTM\",\n",
    "    Prediction=\"CTC\",\n",
    "    num_fiducial=20,\n",
    "    input_channel=1,\n",
    "    output_channel=512,\n",
    "    hidden_size=256\n",
    ")\n",
    "\n",
    "\"\"\" vocab / character number configuration \"\"\"\n",
    "if opt.sensitive:\n",
    "    opt.character = string.printable[:-6]  # same with ASTER setting (use 94 char).\n",
    "\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = True\n",
    "opt.num_gpu = torch.cuda.device_count()\n",
    "# print (opt.image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Open csv file wherein you are going to write the Predicted Words\"\"\"\n",
    "# data = pd.read_csv(opt.image_folder + 'data.csv')\n",
    "\n",
    "def extract_text(image, bboxes):\n",
    "\n",
    "    # Save cropped images\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        filename = f\"../Results/cropped/test_{i}.png\"\n",
    "        top_left, bottom_right = bbox\n",
    "        cv2.imwrite(filename, image[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0], :])\n",
    "\n",
    "    extracted_texts = []\n",
    "\n",
    "    \"\"\" model configuration \"\"\"\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        converter = CTCLabelConverter(opt.character)\n",
    "    else:\n",
    "        converter = AttnLabelConverter(opt.character)\n",
    "#     print(converter.character[:37])\n",
    "    opt.num_class = len(converter.character)\n",
    "\n",
    "    if opt.rgb:\n",
    "        opt.input_channel = 3\n",
    "    model = Model(opt)\n",
    "#     print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
    "#           opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
    "#           opt.SequenceModeling, opt.Prediction)\n",
    "    model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "    # load model\n",
    "#     print('loading pretrained model from %s' % opt.saved_model)\n",
    "    model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n",
    "\n",
    "    # prepare data. two demo images from https://github.com/bgshih/crnn#run-demo\n",
    "    AlignCollate_demo = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
    "    demo_data = RawDataset(root=opt.image_folder, opt=opt)  # use RawDataset\n",
    "    demo_loader = torch.utils.data.DataLoader(\n",
    "        demo_data, batch_size=opt.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=int(opt.workers),\n",
    "        collate_fn=AlignCollate_demo, pin_memory=True)\n",
    "\n",
    "    # predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image_tensors, image_path_list in demo_loader:\n",
    "\n",
    "            batch_size = image_tensors.size(0)\n",
    "            image = image_tensors.to(device)\n",
    "            # For max length prediction\n",
    "            length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
    "            text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "\n",
    "            if 'CTC' in opt.Prediction:\n",
    "                preds = model(image, text_for_pred)\n",
    "\n",
    "                # Select max probabilty (greedy decoding) then decode index to character\n",
    "                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                _, preds_index = preds.max(2)\n",
    "                # preds_index = preds_index.view(-1)\n",
    "                preds_str = converter.decode(preds_index.data, preds_size.data)\n",
    "\n",
    "            else:\n",
    "                preds = model(image, text_for_pred, is_train=False)\n",
    "\n",
    "                # select max probabilty (greedy decoding) then decode index to character\n",
    "                _, preds_index = preds.max(2)\n",
    "                preds_str = converter.decode(preds_index, length_for_pred)\n",
    "\n",
    "            dashed_line = '-' * 80\n",
    "            head = f'{\"image_path\":25s}\\t {\"predicted_labels\":25s}\\t confidence score'\n",
    "\n",
    "#             print(f'{dashed_line}\\n{head}\\n{dashed_line}')\n",
    "            # log.write(f'{dashed_line}\\n{head}\\n{dashed_line}\\n')\n",
    "\n",
    "            preds_prob = F.softmax(preds, dim=2)\n",
    "            preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "            for img_name, pred, pred_max_prob in zip(image_path_list, preds_str, preds_max_prob):\n",
    "\n",
    "\n",
    "                start = opt.image_folder\n",
    "                path = os.path.relpath(img_name, start)\n",
    "\n",
    "                folder = os.path.dirname(path)\n",
    "\n",
    "                image_name=os.path.basename(path)\n",
    "\n",
    "                file_name='_'.join(image_name.split('_')[:-8])\n",
    "\n",
    "                txt_file=os.path.join(start, folder, file_name)                \n",
    "\n",
    "                if 'Attn' in opt.Prediction:\n",
    "                    pred_EOS = pred.find('[s]')\n",
    "                    pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "                    pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "\n",
    "                # calculate confidence score (= multiply of pred_max_prob)\n",
    "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "                extracted_texts.append([pred, confidence_score])\n",
    "#                 print(f'{image_name:25s}\\t {pred:25s}\\t {confidence_score:0.4f}')\n",
    "\n",
    "    # Save cropped images\n",
    "    for i in range(len(bboxes)):\n",
    "        filename = f\"../Results/cropped/test_{i}.png\"\n",
    "        os.remove(filename)\n",
    "    \n",
    "    return extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515c7165579943bbaae9832fe66aa7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['accent', tensor(0.4214, device='cuda:0')], ['loginn', tensor(0.1980, device='cuda:0')], ['withh', tensor(0.0799, device='cuda:0')], ['g', tensor(0.0070, device='cuda:0')], ['t', tensor(0.2390, device='cuda:0')], ['orcreate', tensor(0.5701, device='cuda:0')], ['neww', tensor(0.7467, device='cuda:0')], ['account', tensor(0.4195, device='cuda:0')], ['emailaddress', tensor(0.4479, device='cuda:0')], ['o', tensor(0.2487, device='cuda:0')], ['password', tensor(0.8193, device='cuda:0')], ['oi', tensor(0.2006, device='cuda:0')], ['contimpassword', tensor(0.3247, device='cuda:0')], ['create', tensor(0.8140, device='cuda:0')], ['account', tensor(0.5874, device='cuda:0')], ['already', tensor(0.8242, device='cuda:0')], ['lnae', tensor(0.0092, device='cuda:0')], ['encome', tensor(0.0001, device='cuda:0')], ['oginnere', tensor(0.5913, device='cuda:0')], ['es', tensor(4.5099e-09, device='cuda:0')]]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "I_copy = imgproc.loadImage(image_list[1])\n",
    "bboxes = []\n",
    "for i, key in enumerate(data[\"word_bboxes\"][1]):\n",
    "    top_left = tuple([int(x) for x in data[\"word_bboxes\"][1][key][0]])\n",
    "    bottom_right = tuple([int(x) for x in data[\"word_bboxes\"][1][key][2]])\n",
    "    bboxes.append([top_left, bottom_right])\n",
    "#     print(tuple(top_left.tolist()), tybottom_right.tolist())\n",
    "    cv2.rectangle(I_copy, top_left, bottom_right, 255, 2)\n",
    "#     cv2.imwrite(f\"../Results/cropped/test_{i}.png\", \n",
    "#                 data[\"image_data\"][0][top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]])\n",
    "#     plt.imshow(I_copy[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]])\n",
    "plt.imshow(I_copy)\n",
    "plt.show()\n",
    "# bboxes.append([[211,0], [1620,23]])\n",
    "print(extract_text(I_copy, bboxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
